\documentclass{paper}

\usepackage{parskip}
\usepackage[hidelinks,
	pdftitle={Additional Security by Hashing Passwords in Browsers},
	pdfauthor={Luc Gommans}
	]{hyperref}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{longtable}

\usepackage{float}
\floatstyle{boxed} 
\restylefloat{figure}
\frenchspacing

\newcommand{\code}[1]{\texttt{\colorbox{gray!11}{#1}}}

\begin{document}
\title{Additional Security by Hashing Passwords in Browsers}
\author{By \href{https://lgms.nl}{Luc Gommans}}

\maketitle

Status: draft, version 0.4.

\section*{Abstract}

This paper proposes hashing passwords in the browser, as a security measure additional to
hashing server-side. Since users re-use passwords and do not universally use a password
manager (making it hard to change passwords upon compromise), passwords should never leave
a user's device unprotectd. Hashing in browsers will prevent leaking passwords in a variety
of scenarios which are discussed in section 1. The algorithm to be used by browsers will be
discussed in section 2. Section 3 discusses the HTML interface which is to be be provided to
web developers. In section 4 consequences of the proposed changes are identified and used to
further improve security. Finally in section 5 we will discuss future work. The paper
concludes with an overview of the proposal.

\section{Threat model}

People often reuse passwords, or even if they use a unique password, remember the password
rather than using a password manager. This makes passwords hard to change in the event of a
compromised website.

It is best practice to use password hashing algorithms when storing passwords in a database,
which prevents attackers from being able to obtain the original password if the database is
compromised. Still, there have been various events in the past where plaintext passwords were
compromised due to transport layer security (TLS) being the only protection, or due to bad
password hashing on the server's side (or even none at all). Passwords should never leave a
device unprotected and client-side hashing will additionally show users that this website
will store their password in a secured way.

If client-side hashing was implemented, the server would be the entity which sends the
(Javascript) instructions to do so. If the server was compromised or otherwise not trusted,
it would not be sending your browser instructions to hash the password. Therefore web
developers used to argue that client-side hashing added no value over TLS: if the server is
untrusted, you would not trust its Javascript; and if the server is trusted, there is no
point in hashing either because you trust it to handle your login correctly. As attacks such
as Cloudbleed show, this is a false presumption.

This paper proposes to hash passwords client-side despite the aforementioned required trust
of the server. This will protect against:

\begin{itemize}
\item vulnerabilities such as
% Had to inline Heartbleed, can't do two \footnotemarks without \footnotetext in between
Heartbleed\footnote{\href{https://en.wikipedia.org/wiki/Heartbleed}{\underline{\smash{en.wikipedia.org/wiki/Heartbleed}}}}
and Cloudbleed\footnote{{\href{https://en.wikipedia.org/wiki/Cloudbleed}{\underline{\smash{en.wikipedia.org/wiki/Cloudbleed}}}}} where server memory was accidentally leaked;

\item protect against passive attackers which attack websites without TLS encryption, or
websites that use early TLS termination\footnotemark; and

\item weak or no password hashing on the server.
\end{itemize}

\footnotetext{Early TLS Termination is a technique where TLS traffic is decrypted by a
\href{https://en.wikipedia.org/wiki/TLS\_termination\_proxy}{\underline{\smash{TLS
termination proxy}}} before it reaches its destination, usually at the edge of a private
network, after which it is assumed to be secure. A well known example of abuse of this
scenario is the NSA's tapping of Google's private network
(\href{http://www.androidauthority.com/ssl-added-removed-google-mocks-nsa-crypto-code-easter-egg-390162/}{\underline{\smash{more info}}}).}

One attack it does not protect against is pass-the-hash. For this reason it is still
recommended to hash passwords server-side, regardless of whether passwords are hashed
client-side.

In section 4 it is described how the proposed solution can also partially protect against an
untrustworthy server, for example in the even of phishing, and man-in-the-middle attacks with
active adversaries (as opposed to mere passive interception).

\section{Choice of algorithms}

Passwords should be hashed using a slow algorithm which accepts a salt as input to make the
output unique. However, the server will not have a copy of the plaintext of the password,
making it impossible to use a standard technique.

To generate the salt, we need to use a key derivation function with globally unique inputs. A
combination of the username and website's domain are good candidates as inputs, but a
website's domain is not necessarily stable. There might be multiple, for example one with
\code{www.} included and one without, and it might change over time. Because of this, a
service identifier needs to be supplied by the developers, which is an identifying string
reused across all websites that are linked to the same database. Section 3 will explain this
service identifier further. The risks and benefits of service identifiers will be discussed
in section 4.

A salt needs to be unique, but is not secret nor does it matter if it can be reversed. A fast
hashing algorithm can be used for this purpose. For version 1, the KDF algorithm is an HMAC
of the service identifier and the username, in that order.

\code{KDF = HMAC(service-identifier, username)}

HMAC is used as defined in RFC 2104\footnotemark. The underlying hashing algorithm to be used is SHA-256.
The inputs are encoded as UTF-8.

\footnotetext{\href{https://tools.ietf.org/html/rfc2104}{\underline{\smash{tools.ietf.org/html/rfc2104}}}
HMAC}

The hashing algorithm to be used for password hashing is PBKDF2, as defined in RFC
2898\footnotemark, with an iteration count of 30 000 and derived key length of 32 bytes (64
hexadecimal characters). The iteration count is based on it taking just below 200
milliseconds on a reasonably modern mobile phone: quick enough to not be very noticeable on
the vast majority of devices and, given that it is not an action that needs to be performed
often, acceptable on slower devices.

\footnotetext{\href{https://tools.ietf.org/html/rfc2898\#section-5.1}{\underline{\smash{tools.ietf.org/html/rfc2898\#section-5.1}}}
PBKDF2}

\subsection{Upgrading the algorithm}

As computers get faster and cryptography advances, the algorithm will have to be replaced
at some point in the future. Unfortunately no well-reviewed password hashing algorithm
supports seamless upgrades, where the iteration count can be upgraded without requiring user
input. Rather than proposing a completely new password hashing algorithm, we use an existing
one (PBKDF2) and provide two possible upgrade schemes.

\subsubsection{Wrapping algorithms}

If the old version is simply too fast and is not considered to introduce any weakness, a new
version could wrap an old version to make it slower. This has the great advantage for web
developers that the password database can be upgraded without any user input.

Whether this can be used will depend on the specification of version 2.

\subsubsection{Replacing algorithms}

To completely replace an old algorithm, websites can ask the browser to send both the new and
the old hash to the server. This allows the website to validate the old hash and, if
successful, store the new one. The major downside is that this requires every user to log in
during the transitioning period.

In this event, the browser will send the new version concatenated with the old version,
separated by a dollar sign. For example, when upgrading from version 1 to version 2, this
could result in the following string:

\code{hashed\$v2\$2b00042f7481c7b056c4b\$hashed\$v1\$b60ca610c6aa34ccb0f7}

The exact HTML interface is defined in section 3.

\section{HTML API}

The HTML API for this feature consists of four new attributes for the \code{input} element
with type \code{password}.

\begin{itemize}
\item \code{hash} is required and contains the version to be used. Currently only version 1
is valid, denoted as \code{v1}.
\item \code{username-field} is required and should point to the \code{name} of the username
field in the same form. It defaults to the value \code{username}.
\item \code{service} is required and should be set to the service for which the client is
encrypting a hash. It is recommended to use your service's domain name here, e.g.
\code{example.com}. Elsewhere in the document, this parameter is referred to as the service
identifier.
\item \code{upgrade-from} can be used to upgrade an older version. Legal values are all legal
values for the \code{hash} attribute. When specified, the browser must send both the new and
the old version, in that order.
\end{itemize}

If an unknown version is specified, the browser must issue a warning and should fall back to
the nearest known version, preferring the higher value in case of a tie. If a required
attribute is missing or if \code{username-field} does not point to an existing field, the
browser must issue a warning and should send the string \code{error-hashing!} concatenated
with a random alphanumeric string of eight characters.

The reason for sending a random string is to make certain that the server cannot possibly
process the data correctly. If we would omit the field, languages such as PHP would issue a
NOTICE-level warning (often not shown) and continue with an empty string, which would have
the effect of registering users with empty passwords. If we would fill the field with only a
static string, users might all register and be able to sign in with {\it any} password, since
they all turn into the same, static string.

The reason for not providing defaults for some required fields is because specifying the
\code{hash} field indicates that the developer intends to better secure the page. If a
mistake is made which would (partially) compromise this security, it is considered better to
fail rather than to silently weaken the security.

This results in the following example HTML form. Note that this is the same for both
registering and logging in.

\code{<form method=post action="/login">} \\
\code{Username: <input name=MyUsername>} \\
\code{Password: <input type=password name=MyPassword hash=v1} \\
\code{ service=example.org username-field=MyUsername>} \\
\code{<input type=submit>} \\
\code{</form>}

On the server side, the \code{MyUsername} field is unaltered. The \code{MyPassword} field
contains a string in the following format for version 1:

\code{hashed\$version\$hash}

where \code{hashed} is literal; \code{version} is the value from the \code{hash} attribute;
and \code{hash} is 64 hexadecimal characters. For example:

\code{hashed\$v1\$202d6132b2d2a469607f093dfef14e0f4798956dacbffe615453e00} \\
\code{8d190c861}

Browsers which do not support these attributes will not react to any of the parameters and
send the password in plain text instead. This can be detected because the value submitted to
the server does not begin with \code{hashed\$v1\$}.

It is recommended to detect this If an unhashed password is submitted, the server should hash
the password as the browser should have done and continue as normal.

Example implementations are provided in the source code repository at \\
{\href{https://github.com/lgommans/browserhashing}{\underline{\smash{github.com/lgommans/browserhashing}}}}.

\section{Additional effects}

Allowing the website to supply the service identifier, rather than deriving it from the
domain or other semi-static field, allows attackers to supply the service identifier of their
intended targets. For example, a phishing website which tries to obtain logins for Reddit
would supply the same identifier as Reddit does.

Rather than being a risk, this can be used to improve security: duplicate service identifiers
can be logged to a public log. Reddit could subscribe itself to the log, to be notified
when duplicates are discovered. This would allow the website to spot domains falsely
identifying as Reddit before the first victim was even able to enter their password.

In this scenario, it is assumed that the user has visited the website before. If the user
lands on a phishing page for a website they never visited before, they probably do not have
credentials to be phished. Basically this is an implicit Trust On First Use-scheme, but
distributed through all the users of browsers participating in logging to the public log.
This log could be a project such as Microsoft SmartScreen or Google Safe Browsing.

Browsers could also issue visible warnings to users, notifying them that this website on
domain X attempts to obtain their password for this other website from domain Y, and asking
whether they want to continue. Communicating this clearly does provide UX challenges and is a
future step which is out of scope for this paper.

Finally, once many websites adopted this scheme, browsers could issue a warning (similar to
the one given on unencrypted HTTP pages with a password field) when a page has a password
field that does not include hashing. This forces websites to use the scheme if they do not
want to be marked as insecure, which means phishing websites must also do it. Combined with
the aforementioned duplicate service identifier detection, this would be an effective measure
against phishing.

\section{Future work}

As discussed in section 2.1 (Upgrading the algorithm), upgrades will definitely be necessary
in the future. The current cost (an iteration count of 30 000) is chosen to err on the low
side and the algorithm (PBKDF2) is chosen for its widespread support and simplicity, both to
make sure the proposed feature will not be in the way of usability for users and web
developers. If it is found that practically no device experiences slowdowns from the proposed
feature, version 2 of this scheme could drastically increase the cost and perhaps upgrade to
more complex algorithms, possibly one that is memory-hard as well.

While a second version is being designed, multiple future versions should be designed as
well. This is for three reasons:

\begin{enumerate}
\item the upgrade process would be very simple if an old version can simply be upgraded by
applying extra iterations server-side (as opposed to having to ask users to log in to upgrade
their hashes);
\item it would allow browser vendors to implement multiple future versions at the same time,
even if some versions are only meant to be used years ahead, preventing issues with browser
support for new versions once it is time to upgrade to them; and
\item it would allow developers that target a specific audience to use versions that are
currently too slow for general (e.g. mobile) use, for example if an internal website is only
going to be used on desktops, or if security requirements are very strict (e.g. governments).
\end{enumerate}

\section*{Conclusion}

In this paper we proposed additional attributes to the HTML password input field which
instruct the browser to hash a user's password. Since all browsers must hash the same way,
servers can treat the new input as a simple substitute for the original password.

Until this feature has widespread support, the only necessary change is to check whether an
old-style (plaintext) or a new-style (hashed) password was submitted. In the former case, the
server can hash it like the browser should have done and continue normally.

One of the proposed attributes is a \code{service} attribute, the service identifier, which
identifies the service that passwords are being hashed for. This has the function of making a
hash unique per website.

Websites with multiple domains can use the same service identifier, thereby being able to use
the same user database without any issues. This has the additional advantage that phishing
websites, which attempt to obtain logins for other websites, must use the same service
identifier or the hash they receive will be useless. Browsers could detect different domains
that use the same service identifier and take action by warning users and submitting the
domain to projects such as Google Safe Browsing and Microsoft SmartScreen.

The scheme protects against the leakage of plaintext credentials in events such as Heartbleed
and Cloudbleed; shows users that their passwords are not stored in plain text; and provides
another measure against phishing attacks.

\section*{Acknowledgments}

Many thanks for early feedback from Khaled Nassar, Paul Cornelissen, Koen Tange, Bart Erven
and Fr\'ed\'eric Schertenleib.

\end{document}

